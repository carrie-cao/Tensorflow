{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape the data \n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
    "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "    \n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images /= 255\n",
    "test_images /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
    "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATPUlEQVR4nO3dfZBddX3H8fcnBAKSoIRsQoCQ1RAo1Gqgl9QZosBQMEQUsFXA4gCVxhlEzQxlDGkFlExlOkaxINBAUqIICoWQkIaWGBiUMlAWxDw0liejBEKyeSIhRDDh2z/OWbnZ3Ht29967e6/8Pq+ZO3v3fM+553vP7mfP0z17FBGY2bvfoGY3YGYDw2E3S4TDbpYIh90sEQ67WSIcdrNEOOwtSNLVkm5vdh+tqJ5lk/pyddjLSJok6TFJr0naJOm/JR3f7L7qIelSSR2S3pR0W7faRyQtyd9rp6S7JY0uq0+T9KKkrZJekfRdSYMrzONESSFpZh/6uq0v4w80Scfky21z/vippGOa3Vc9HPacpAOARcD1wHDgUOAbwJvN7KsBXgFmAnMr1A4EZgPtwFhgG/BvZfX7geMi4gDgg8CHga+Uv4CkvYHvAU80uvEmewX4a7LfhRHAQuDHTe2oTg77O44EiIg7I2JXROyIiAcjYhmApHGSHpK0UdIGST+S9L6uiSWtlnS5pGWStkuaI2mUpAckbcvXDAfm47bna8Kp+RpzraTLqjWWr4Efk7RF0i8lndTbNxUR90bEfcDGCrUHIuLuiNgaEW8ANwAnlNVfiIgtXW0AbwNHdHuZy4AHgV/1tqeeSPqepJfyLYqnJH202yj7SvpJvlyflvThsmkPkXRPvqXya0lfoQYRsSUiVkf2EVMBu9jzvf9Rcdjf8SywS9I8Sad3BbOMgG8BhwBHA2OAq7uN81fAqWR/OD4JPADMIFszDKLbWhE4GRgPnAZMl/SX3ZuSdCjwH2Rr5+HA3wP3SGrL69MlLarlDVfwMWBlt/l/TtJWYAPZmv1fy2pjgb8Fvtmg+Xd5EphA9n7vAO6WtG9Z/Uzg7rL6fZL2ljSIbGvkl2RbZqcA0yR9vNJM8j/MnytqRNIW4HdkW3z/VNe7ajKHPRcRW4FJQAC3AJ2SFkoaldefj4glEfFmRHQC3wFO7PYy10fEuoh4Gfg58ERE/CIi3gTmA8d2G/8bEbE9IpaTbT6fV6G184HFEbE4It6OiCVABzAl7+vaiDij3vcv6UPAlcDl5cMj4o58M/5I4GZgXVn5X4CvR8Tr9c6/2zxvj4iNEbEzImYBQ4CjykZ5KiL+PSJ+T/Zz2Bf4CHA80BYR34yItyLiRbKf5blV5vOhiLijh17eB7wXuBT4Rd1vrokc9jIRsSoiLoyIw8j2UQ8BrgOQNFLSjyW9nK/pbidbY5crD8KOCt8P7Tb+S2XPf5PPr7uxwGfyTfgt+ZpmEjC6wrg1kXQE2VbIVyPi55XGiYjnyNb6N+bTfBIYFhE/aVQfZf1cJmlVfqB0C1nYypf1H5ZbRLwNrCFbdmOBQ7otqxnAqHr6iYjtZH/ofiBpZD2v1Ux7HFm1TET8Kj96/cV80LfI1vofioiNks4i28etxxje2dc9nOygUHcvAT+MiL+rc14V5ZviPwWuiYgf9jD6YGBc/vwUoCTp1fz795LtBv1ZRJxZRz8fBb6Wv/7KiHhb0may3aguY8rGHwQcRrbsdgK/jojxtc6/wCDgPWS7B+v74fX7ndfsOUl/kq9RDsu/H0O2Wf14Psow4HVgS74ffXnlV+qTr0t6j6Q/BS4CKq0lbwc+KenjkvaStK+kk7r67Imkwfn+7l5A1/SD89qhwEPA9yPi5grTXty1JstPO10BLO3qnWzTfkL+WEi2yXxRr9/9O/10PfYhW847gU5gsKQrgQO6Tffnkj6dv49pZGdMHgf+B9gq6WuS9suX1wdVw+lTSadKOjZ/jQPIdhc2A6v6+lqtwmF/xzbgL4AnJG0n++VZQXa0GbLTcMcBr5EdMLu3AfN8BHieLEDfjogHu48QES+RHZCaQRaAl8j+0AwCkDRD0gMF8/hHsl2I6WT7/zvyYQAXAx8ArpL0etejbNoTgOX58licP2bkfW2LiFe7Hvnrbo+ITX14/9Pz6boeDwH/RbZL8SzZrs3v2H13B2ABcA5Z+D4PfDoifh8Ru8gOjE4Afk12UPFWsq2OPUhaKelvqvT2PuBOsp/3C2RH4idHxO/68P5aivzPKwaepHayX8a9I2Jnc7uxVHjNbpYIh90sEd6MN0uE1+xmiRjQ8+wjRoyI9vb2gZylWVJWr17Nhg0bVKlWV9glTSa74mkv4NaIuLZo/Pb2djo6OuqZpZkVKJVKVWs1b8ZL2gv4PnA6cAxwnv7Ir/c1ezerZ599IvB8RLwYEW+RXetb88ckzax/1RP2Q9n9k01r8mG7ya/Z7pDU0dnZWcfszKwe9YS90kGAPc7jRcTsiChFRKmtra2O2ZlZPeoJ+xrKrj7inSuPzKwF1RP2J4Hxkt6fX610LtmVT2bWgmo+9RYROyVdSnaV0l7A3IhY2cNkZtYkdZ1nj4iuyx7NrMX547JmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYI37L5Xe6SSy4prN90002F9SuvvLKwfv755xfWx4/vj7snWy28ZjdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuHz7ImTKt7d9w9mzpxZWL/rrrsK67fcckvV2vHHH1847ZAhQwrr1jdes5slwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmifB59ne5iy66qK7p58yZU1h/9tlnC+snnnhi1dqqVasKpz3yyCML69Y3dYVd0mpgG7AL2BkRpUY0ZWaN14g1+8kRsaEBr2Nm/cj77GaJqDfsATwo6SlJUyuNIGmqpA5JHZ2dnXXOzsxqVW/YT4iI44DTgS9J+lj3ESJidkSUIqLU1tZW5+zMrFZ1hT0iXsm/rgfmAxMb0ZSZNV7NYZe0v6RhXc+B04AVjWrMzBqrnqPxo4D5+fXQg4E7IuI/G9KVNUxP14z3VB86dGhhfdasWX3uqcvll19eWF+wYEHNr217qjnsEfEi8OEG9mJm/cin3swS4bCbJcJhN0uEw26WCIfdLBG+xNUKXXPNNYX1/fbbr7Be9K+oH3roocJpH3744cL6ySefXFi33XnNbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwufZrVBPt02+8MILC+tF59nfeOONwml37NhRWLe+8ZrdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEz7Nboeuuu66wPnfu3Jpf++ijjy6sH3XUUTW/tu3Ja3azRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBE+z/4usGTJkqq1G264oXDaRx55pLDe0zXlO3fuLKwXGTduXF1165se1+yS5kpaL2lF2bDhkpZIei7/emD/tmlm9erNZvxtwORuw6YDSyNiPLA0/97MWliPYY+InwGbug0+E5iXP58HnNXgvsyswWo9QDcqItYC5F9HVhtR0lRJHZI6Ojs7a5ydmdWr34/GR8TsiChFRKmtra2/Z2dmVdQa9nWSRgPkX9c3riUz6w+1hn0hcEH+/AJgQWPaMbP+0uN5dkl3AicBIyStAa4CrgXukvQF4LfAZ/qzSStW9L/ZH3300cJpI6KwLqmwPmzYsML6okWLqtYOOuigwmmtsXoMe0ScV6V0SoN7MbN+5I/LmiXCYTdLhMNulgiH3SwRDrtZInyJq9XlrbfeKqxv3Lixam3SpEmNbscKeM1ulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXC59nfBXr6d9BFLrnkksL6q6++Wli/7777Cutnn3121doZZ5xROO3ChQsL69Y3XrObJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZonwefbE3XjjjYX17du3F9bPPffcwvrixYur1jZv3lw47aZN3W8xuLvhw4cX1m13XrObJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZonweXYrtP/++xfWp02bVlgvOs/+2GOPFU77+OOPF9anTJlSWLfd9bhmlzRX0npJK8qGXS3pZUnP5A8vdbMW15vN+NuAyRWGfzciJuSP6n++zawl9Bj2iPgZUPy5RTNrefUcoLtU0rJ8M//AaiNJmiqpQ1JHZ2dnHbMzs3rUGvabgHHABGAtMKvaiBExOyJKEVFqa2urcXZmVq+awh4R6yJiV0S8DdwCTGxsW2bWaDWFXdLosm/PBlZUG9fMWkOP59kl3QmcBIyQtAa4CjhJ0gQggNXAF/uxR2thpVKp2S1YL/UY9og4r8LgOf3Qi5n1I39c1iwRDrtZIhx2s0Q47GaJcNjNEuFLXAfAjh07Cus9XSY6a1bVDygCMHTo0D731CjLly9v2rytb7xmN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4fPsDdDTefQrrriisH7rrbcW1g8++ODC+owZM6rWhgwZUjhtvW6++eaap504sfh/nvjy2cbymt0sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TPszfA0qVLC+vXX399Xa8/c+bMwvqpp55atTZp0qTCaYvO0ffGsmXLap724osvLqyPHDmy5te2PXnNbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJsloje3bB4D/AA4GHgbmB0R35M0HPgJ0E522+bPRsTm/mu1dU2ePLmw/sILLxTWP/WpTxXWV65cWVj/xCc+UbU2aFDx3/PXXnutsC6psF6P0047rd9e2/bUmzX7TuCyiDga+AjwJUnHANOBpRExHliaf29mLarHsEfE2oh4On++DVgFHAqcCczLR5sHnNVfTZpZ/fq0zy6pHTgWeAIYFRFrIfuDAPizjWYtrNdhlzQUuAeYFhFb+zDdVEkdkjo6Oztr6dHMGqBXYZe0N1nQfxQR9+aD10kanddHA+srTRsRsyOiFBGltra2RvRsZjXoMezKDsfOAVZFxHfKSguBC/LnFwALGt+emTVKby5xPQH4PLBc0jP5sBnAtcBdkr4A/Bb4TP+02PoGDy5ejO3t7YX1+++/v7A+f/78wvpVV11VtbZ1a6/3uGpy+OGHF9bPOeecqjVfwjqwegx7RDwKVDvZekpj2zGz/uJP0JklwmE3S4TDbpYIh90sEQ67WSIcdrNE+F9Jt4CxY8cW1qdNm1ZY32effarWvvzlL9fUU5fx48cX1hctWlRYP+KII+qavzWO1+xmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIUEQM2s1KpFB0dHQM2P7PUlEolOjo6Kl6S7jW7WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpaIHsMuaYykhyWtkrRS0lfz4VdLelnSM/ljSv+3a2a16s1NInYCl0XE05KGAU9JWpLXvhsR3+6/9sysUXoMe0SsBdbmz7dJWgUc2t+NmVlj9WmfXVI7cCzwRD7oUknLJM2VdGCVaaZK6pDU0dnZWVezZla7Xodd0lDgHmBaRGwFbgLGARPI1vyzKk0XEbMjohQRpba2tga0bGa16FXYJe1NFvQfRcS9ABGxLiJ2RcTbwC3AxP5r08zq1Zuj8QLmAKsi4jtlw0eXjXY2sKLx7ZlZo/TmaPwJwOeB5ZKeyYfNAM6TNAEIYDXwxX7p0MwaojdH4x8FKv0f6sWNb8fM+os/QWeWCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0SoYgYuJlJncBvygaNADYMWAN906q9tWpf4N5q1cjexkZExf//NqBh32PmUkdElJrWQIFW7a1V+wL3VquB6s2b8WaJcNjNEtHssM9u8vyLtGpvrdoXuLdaDUhvTd1nN7OB0+w1u5kNEIfdLBFNCbukyZL+T9LzkqY3o4dqJK2WtDy/DXVHk3uZK2m9pBVlw4ZLWiLpufxrxXvsNam3lriNd8Ftxpu67Jp9+/MB32eXtBfwLHAqsAZ4EjgvIv53QBupQtJqoBQRTf8AhqSPAa8DP4iID+bD/hnYFBHX5n8oD4yIr7VIb1cDrzf7Nt753YpGl99mHDgLuJAmLruCvj7LACy3ZqzZJwLPR8SLEfEW8GPgzCb00fIi4mfApm6DzwTm5c/nkf2yDLgqvbWEiFgbEU/nz7cBXbcZb+qyK+hrQDQj7IcCL5V9v4bWut97AA9KekrS1GY3U8GoiFgL2S8PMLLJ/XTX4228B1K324y3zLKr5fbn9WpG2CvdSqqVzv+dEBHHAacDX8o3V613enUb74FS4TbjLaHW25/XqxlhXwOMKfv+MOCVJvRRUUS8kn9dD8yn9W5Fva7rDrr51/VN7ucPWuk23pVuM04LLLtm3v68GWF/Ehgv6f2S9gHOBRY2oY89SNo/P3CCpP2B02i9W1EvBC7In18ALGhiL7tpldt4V7vNOE1edk2//XlEDPgDmEJ2RP4F4B+a0UOVvj4A/DJ/rGx2b8CdZJt1vyfbIvoCcBCwFHgu/zq8hXr7IbAcWEYWrNFN6m0S2a7hMuCZ/DGl2cuuoK8BWW7+uKxZIvwJOrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEf8PHScMd08W7H8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check images with its label:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample(num):\n",
    "    #Print the one-hot array of this sample's label \n",
    "    print(train_labels[num])  \n",
    "    #Print the label converted back to a number\n",
    "    label = train_labels[num].argmax(axis=0)\n",
    "    #Reshape the 768 values to a 28x28 image\n",
    "    image = train_images[num].reshape([28,28])\n",
    "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "    \n",
    "display_sample(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a convolutional neural network involves more layers. \n",
    "Start with a 2D convolution of the image \n",
    "Then run a second convolution on top of that with 64 3x3 windows \n",
    "Next apply a MaxPooling2D layer that takes the maximum of each 2x2 result to distill the results down into something more manageable.\n",
    "A dropout filter is then applied to prevent overfitting.\n",
    "Next we flatten the 2D layer into a 1D layer \n",
    "Apply dropout again to further prevent overfitting.\n",
    "Feed that into final 10 units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "# 64 3x3 kernels\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# Reduce by taking the max of each 2x2 block\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Dropout to avoid overfitting\n",
    "model.add(Dropout(0.25))\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "# A hidden layer to learn with\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Another dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Final categorization from 0-9 with softmax\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#check the model description:\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical_crossentropy is the right loss function for  multiple categorization. Use the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 51s - loss: 0.1901 - accuracy: 0.9423 - val_loss: 0.0460 - val_accuracy: 0.9856\n",
      "Epoch 2/10\n",
      "1875/1875 - 53s - loss: 0.0804 - accuracy: 0.9761 - val_loss: 0.0368 - val_accuracy: 0.9867\n",
      "Epoch 3/10\n",
      "1875/1875 - 53s - loss: 0.0593 - accuracy: 0.9817 - val_loss: 0.0309 - val_accuracy: 0.9892\n",
      "Epoch 4/10\n",
      "1875/1875 - 53s - loss: 0.0482 - accuracy: 0.9850 - val_loss: 0.0315 - val_accuracy: 0.9902\n",
      "Epoch 5/10\n",
      "1875/1875 - 60s - loss: 0.0427 - accuracy: 0.9871 - val_loss: 0.0313 - val_accuracy: 0.9898\n",
      "Epoch 6/10\n",
      "1875/1875 - 53s - loss: 0.0344 - accuracy: 0.9895 - val_loss: 0.0285 - val_accuracy: 0.9916\n",
      "Epoch 7/10\n",
      "1875/1875 - 52s - loss: 0.0308 - accuracy: 0.9899 - val_loss: 0.0274 - val_accuracy: 0.9922\n",
      "Epoch 8/10\n",
      "1875/1875 - 52s - loss: 0.0289 - accuracy: 0.9907 - val_loss: 0.0271 - val_accuracy: 0.9925\n",
      "Epoch 9/10\n",
      "1875/1875 - 52s - loss: 0.0266 - accuracy: 0.9916 - val_loss: 0.0286 - val_accuracy: 0.9915\n",
      "Epoch 10/10\n",
      "1875/1875 - 52s - loss: 0.0225 - accuracy: 0.9929 - val_loss: 0.0289 - val_accuracy: 0.9933\n",
      "00:08:51.36\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "#timing\n",
    "import time\n",
    "start = time.time()\n",
    "history = model.fit(train_images, train_labels,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_images, test_labels))\n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.034049834153382426\n",
      "Test accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
